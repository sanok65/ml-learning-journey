{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Intro to Numpy\n\nThe notebook demonstrates NumPy's efficiency for mathematical operations like array reshaping, sigmoid, softmax, dot and outer products, L1 and L2 losses, and matrix operations. It highlights NumPy's superiority over standard Python lists in speed and convenience for scientific computing and machine learning tasks.","metadata":{}},{"cell_type":"markdown","source":"### Reshape\nCreate a function that takes a NumPy array of shape (length,width,height) and converts it in to a vector of shape `(length*width*height,1)`. Use the function `array.reshape()` for this.","metadata":{"id":"8VeDg747Pu7E"}},{"cell_type":"code","source":"import numpy as np\n\ndef convert_to_vector(array):\n    vector = array.reshape((array.shape[0] * array.shape[1] * array.shape[2], 1))\n    return vector\n\nsample_array = np.random.random((2, 3, 4))\n\nvector_result = convert_to_vector(sample_array)\n\nprint(\"Original Array:\")\nprint(sample_array)\nprint(\"\\nResulting Vector:\")\nprint(vector_result)\n","metadata":{"id":"miOuUq_GPybj","outputId":"6264ef1f-5933-40eb-edbf-66639ce8a3fe"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Sigmoid\n- Write a function that returns the sigmoid of a real number x. Use `math.exp(x)` for the exponential function. `Sigmoid(x)=1/(1+exp(-x))`.\n- Now create a list of 5 values and call your sigmoid function with the list as input. You will get an error because `math.exp()` only works when input is a real number. It does not work with vectors and matrices. Now create a new function for sigmoid but this time use `np.exp()` instead of `math.exp()`. `np.exp()` works with all types of inputs including real numbers, vectors and matrices. In deep learning we mostly use matrices and vectors. This is why NumPy is more useful. Call your new function with a vector created by `np.array()` function.","metadata":{"id":"eO8RARDMQTY-"}},{"cell_type":"code","source":"import math\n\ndef sigmoid_math_exp(x):\n    return 1 / (1 + math.exp(-x))\n","metadata":{"id":"UrG4-fPvmZOO"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef sigmoid_np_exp(x):\n    return 1 / (1 + np.exp(-x))\n","metadata":{"id":"xjFYL4ldmZRG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"values = [1.0, 2.0, 3.0, 4.0, 5.0]\ntry:\n    sigmoid_result_math_exp = [sigmoid_math_exp(val) for val in values]\nexcept TypeError as e:\n    print(f\"Error: {e}\")\n\nvector_values = np.array(values)\nsigmoid_result_np_exp = sigmoid_np_exp(vector_values)\n\nprint(\"Results using math.exp():\", sigmoid_result_math_exp)\nprint(\"Results using np.exp():\", sigmoid_result_np_exp)","metadata":{"id":"6uUhoYjlQIYW","outputId":"3840d25c-8831-41a8-894c-7c13333850f9"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Softmax\nCreate a function that takes a matrix as input and returns the softmax (by row) of matrix.\nCheck if your function is working correctly by using suitable inputs.","metadata":{"id":"E8vOW5NOncaT"}},{"cell_type":"code","source":"import numpy as np\n\ndef softmax(matrix):\n    exp_matrix = np.exp(matrix)\n\n    row_sums = np.sum(exp_matrix, axis=1, keepdims=True)\n\n    softmax_matrix = exp_matrix / row_sums\n\n    return softmax_matrix\n","metadata":{"id":"BFpk2-ownWTw"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_matrix = np.array([[1.0, 2.0, 3.0],\n                          [4.0, 5.0, 6.0],\n                          [7.0, 8.0, 9.0]])\n\nsoftmax_result = softmax(sample_matrix)\n\nprint(\"Original Matrix:\")\nprint(sample_matrix)\nprint(\"\\nSoftmax Result:\")\nprint(softmax_result)\n","metadata":{"id":"Hke7DMuanm-m","outputId":"dca1420c-06e5-412e-824e-cdd142d89db8"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dot Product\n- Create a function that implements dot product of two vectors. The input to the function should be two standard python lists. Identify the time taken to evaluate the dot product using a particular example of your choice.\n- Now create another function that implements dot product of two vectors using `np.dot()` function. Identify the time taken to evaluate this dot product and compare it with the time taken in part a.","metadata":{"id":"bBDQwgqYnvYi"}},{"cell_type":"code","source":"import time\nimport numpy as np\n\ndef dot_product_python_lists(vector1, vector2):\n    if len(vector1) != len(vector2):\n        raise ValueError(\"Vectors must have the same length\")\n\n    dot_product = sum(x * y for x, y in zip(vector1, vector2))\n    return dot_product\n\ndef dot_product_numpy(vector1, vector2):\n    dot_product = np.dot(vector1, vector2)\n    return dot_product\n","metadata":{"id":"txarKcwIoBhf"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_vector1 = [1, 2, 3, 4, 5]\nexample_vector2 = [5, 4, 3, 2, 1]\n\nstart_time_python_lists = time.time()\nresult_python_lists = dot_product_python_lists(example_vector1, example_vector2)\nend_time_python_lists = time.time()\ntime_taken_python_lists = end_time_python_lists - start_time_python_lists\n\nstart_time_numpy_dot = time.time()\nresult_numpy_dot = dot_product_numpy(np.array(example_vector1), np.array(example_vector2))\nend_time_numpy_dot = time.time()\ntime_taken_numpy_dot = end_time_numpy_dot - start_time_numpy_dot\n\nprint(\"Dot Product using Python Lists:\", result_python_lists)\nprint(\"Time taken with Python Lists:\", time_taken_python_lists, \"seconds\\n\")\n\nprint(\"Dot Product using np.dot():\", result_numpy_dot)\nprint(\"Time taken with np.dot():\", time_taken_numpy_dot, \"seconds\")\n","metadata":{"id":"1yoWAZQFoZ83","outputId":"b21ccde8-0ec8-4a6b-faa6-8f56ed055135"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Outer Product\n- Create a function that implements outer product of two vectors. The input to the function should be two standard python lists. Identify the time taken to evaluate the outer product using a particular example of your choice.\n- Now create another function that implements outer product of two vectors using `np.outer()` function. Identify the time taken to evaluate this dot product and compare it with the time taken in part a.","metadata":{"id":"nHt5_btdodiM"}},{"cell_type":"code","source":"import time\nimport numpy as np\n\ndef outer_product_python_lists(vector1, vector2):\n    outer_product = [[x * y for y in vector2] for x in vector1]\n    return outer_product\n\ndef outer_product_numpy(vector1, vector2):\n    outer_product = np.outer(vector1, vector2)\n    return outer_product\n","metadata":{"id":"0n2cY50sobA1"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_vector1 = [1, 2, 3, 4, 5]\nexample_vector2 = [5, 4, 3, 2, 1]\n\nstart_time_python_lists = time.time()\nresult_python_lists = outer_product_python_lists(example_vector1, example_vector2)\nend_time_python_lists = time.time()\ntime_taken_python_lists = end_time_python_lists - start_time_python_lists\n\nstart_time_numpy_outer = time.time()\nresult_numpy_outer = outer_product_numpy(np.array(example_vector1), np.array(example_vector2))\nend_time_numpy_outer = time.time()\ntime_taken_numpy_outer = end_time_numpy_outer - start_time_numpy_outer\n\nprint(\"Outer Product using Python Lists:\")\nprint(result_python_lists)\nprint(\"Time taken with Python Lists:\", time_taken_python_lists, \"seconds\\n\")\n\nprint(\"Outer Product using np.outer():\")\nprint(result_numpy_outer)\nprint(\"Time taken with np.outer():\", time_taken_numpy_outer, \"seconds\")\n","metadata":{"id":"KLFvaI7AorwR","outputId":"ea05c3da-fc9e-42f5-e535-d0d781bb6587"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Loss Functions:\n- Create a function that takes two vectors in the form of standard python lists and returns the L1 loss.\n- Now create another function that returns L1 loss but uses NumPy arrays instead of standard python list. Compare the two approaches.\n- Create a function that takes two vectors in the form of standard python lists and returns the L2 loss.\n- Now create another function that returns L2 loss but uses NumPy arrays instead of standard python list. Compare the two approaches.","metadata":{"id":"ZO2U0wZOoxBA"}},{"cell_type":"code","source":"import numpy as np\n\ndef l1_loss_python_lists(vector1, vector2):\n    if len(vector1) != len(vector2):\n        raise ValueError(\"Vectors must have the same length\")\n\n    l1_loss = sum(abs(x - y) for x, y in zip(vector1, vector2))\n    return l1_loss\n\ndef l1_loss_numpy(vector1, vector2):\n    if len(vector1) != len(vector2):\n        raise ValueError(\"Vectors must have the same length\")\n\n    l1_loss = np.sum(np.abs(np.array(vector1) - np.array(vector2)))\n    return l1_loss\n\ndef l2_loss_python_lists(vector1, vector2):\n    if len(vector1) != len(vector2):\n        raise ValueError(\"Vectors must have the same length\")\n\n    l2_loss = sum((x - y)**2 for x, y in zip(vector1, vector2))\n    return l2_loss**0.5\n\ndef l2_loss_numpy(vector1, vector2):\n    if len(vector1) != len(vector2):\n        raise ValueError(\"Vectors must have the same length\")\n\n    l2_loss = np.linalg.norm(np.array(vector1) - np.array(vector2))\n    return l2_loss\n","metadata":{"id":"Pu-BznUBosug"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vector1 = [1, 2, 3, 4, 5]\nvector2 = [5, 4, 3, 2, 1]\n\nl1_loss_result_python_lists = l1_loss_python_lists(vector1, vector2)\nprint(\"L1 Loss using Python Lists:\", l1_loss_result_python_lists)\n\nl1_loss_result_numpy = l1_loss_numpy(np.array(vector1), np.array(vector2))\nprint(\"L1 Loss using NumPy Arrays:\", l1_loss_result_numpy)\n\nl2_loss_result_python_lists = l2_loss_python_lists(vector1, vector2)\nprint(\"\\nL2 Loss using Python Lists:\", l2_loss_result_python_lists)\n\nl2_loss_result_numpy = l2_loss_numpy(np.array(vector1), np.array(vector2))\nprint(\"L2 Loss using NumPy Arrays:\", l2_loss_result_numpy)\n","metadata":{"id":"uRSCkR0xpCGZ","outputId":"5cbdc1a9-48ef-4992-f8d0-5f6a2ed5cf45"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Perform Matrix and Matrix Addition:\n- Create a function that performs matrix and matrix addition by using standard python data structures only.\n- Create a function that performs matrix and matrix addition by using NumPy arrays.","metadata":{"id":"l27quLMypT8C"}},{"cell_type":"code","source":"import numpy as np\n\ndef matrix_addition_python(matrix1, matrix2):\n    if len(matrix1) != len(matrix2) or len(matrix1[0]) != len(matrix2[0]):\n        raise ValueError(\"Matrices must have the same dimensions for addition\")\n\n    result_matrix = [[matrix1[i][j] + matrix2[i][j] for j in range(len(matrix1[0]))] for i in range(len(matrix1))]\n    return result_matrix\n\ndef matrix_addition_numpy(matrix1, matrix2):\n    if matrix1.shape != matrix2.shape:\n        raise ValueError(\"Matrices must have the same dimensions for addition\")\n\n    result_matrix = matrix1 + matrix2\n    return result_matrix\n\n","metadata":{"id":"u0_o4UJwpMTx"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matrix1 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nmatrix2 = [[9, 8, 7], [6, 5, 4], [3, 2, 1]]\n\nresult_matrix_python = matrix_addition_python(matrix1, matrix2)\nprint(\"Matrix Addition using Python Lists:\")\nfor row in result_matrix_python:\n    print(row)\n\nresult_matrix_numpy = matrix_addition_numpy(np.array(matrix1), np.array(matrix2))\nprint(\"\\nMatrix Addition using NumPy Arrays:\")\nprint(result_matrix_numpy)\n","metadata":{"id":"QA8Nm1HVpc68","outputId":"87efa371-31a5-43ac-d47a-ec08faf5fa59"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Perform Matrix and Vector Multiplication:\n- Create a function that performs matrix and vector multiplication by using standard python data structures only.\n- Create a function that performs matrix and vector multiplication by using NumPy arrays.","metadata":{"id":"yYbp9TospruS"}},{"cell_type":"code","source":"import numpy as np\n\ndef matrix_vector_multiplication_python(matrix, vector):\n    if len(matrix[0]) != len(vector):\n        raise ValueError(\"Number of columns in the matrix must be equal to the length of the vector\")\n\n    result_vector = [sum(matrix[i][j] * vector[j] for j in range(len(vector))) for i in range(len(matrix))]\n    return result_vector\n\ndef matrix_vector_multiplication_numpy(matrix, vector):\n    if matrix.shape[1] != len(vector):\n        raise ValueError(\"Number of columns in the matrix must be equal to the length of the vector\")\n\n    result_vector = np.dot(matrix, vector)\n    return result_vector\n","metadata":{"id":"lriYRLd8pmgv"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nvector = [2, 3, 4]\n\nresult_vector_python = matrix_vector_multiplication_python(matrix, vector)\nprint(\"Matrix and Vector Multiplication using Python Lists:\", result_vector_python)\n\nresult_vector_numpy = matrix_vector_multiplication_numpy(np.array(matrix), np.array(vector))\nprint(\"Matrix and Vector Multiplication using NumPy Arrays:\", result_vector_numpy)\n","metadata":{"id":"CfGF4-kOqjIi","outputId":"3fd13177-a60d-4495-d366-ecc01f682e2d"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Perform Matrix and Matrix Multiplication:\n- Create a function that performs matrix and matrix multiplication by using standard python data structures only.\n- Create a function that performs matrix and matrix multiplication by using NumPy arrays.","metadata":{"id":"iFVxUJAJqn7B"}},{"cell_type":"code","source":"def matrix_multiplication_python(matrix1, matrix2):\n    if len(matrix1[0]) != len(matrix2):\n        raise ValueError(\"Number of columns in the first matrix must be equal to the number of rows in the second matrix\")\n\n    result_matrix = [[sum(matrix1[i][k] * matrix2[k][j] for k in range(len(matrix2))) for j in range(len(matrix2[0]))] for i in range(len(matrix1))]\n    return result_matrix\n\nimport numpy as np\n\ndef matrix_multiplication_numpy(matrix1, matrix2):\n    if matrix1.shape[1] != matrix2.shape[0]:\n        raise ValueError(\"Number of columns in the first matrix must be equal to the number of rows in the second matrix\")\n\n    result_matrix = np.dot(matrix1, matrix2)\n    return result_matrix\n","metadata":{"id":"kubNkK3Fqkwc"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matrix1 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nmatrix2 = [[9, 8, 7], [6, 5, 4], [3, 2, 1]]\n\nresult_matrix_python = matrix_multiplication_python(matrix1, matrix2)\nprint(\"Matrix and Matrix Multiplication using Python Lists:\")\nfor row in result_matrix_python:\n    print(row)\n\nresult_matrix_numpy = matrix_multiplication_numpy(np.array(matrix1), np.array(matrix2))\nprint(\"\\nMatrix and Matrix Multiplication using NumPy Arrays:\")\nprint(result_matrix_numpy)\n","metadata":{"id":"gZFtvH5XqvBW","outputId":"b14ec753-2dce-482b-914e-2d9ed4b6274a"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Find More Labs**\n\nThis lab is from my Machine Learning Course, that is a part of my [Software Engineering](https://seecs.nust.edu.pk/program/bachelor-of-software-engineering-for-fall-2021-onward) Degree at [NUST](https://nust.edu.pk).\n\nThe content in the provided list of notebooks covers a range of topics in **machine learning** and **data analysis** implemented from scratch or using popular libraries like **NumPy**, **pandas**, **scikit-learn**, **seaborn**, and **matplotlib**. It includes introductory materials on NumPy showcasing its efficiency for mathematical operations, **linear regression**, **logistic regression**, **decision trees**, **K-nearest neighbors (KNN)**, **support vector machines (SVM)**, **Naive Bayes**, **K-means** clustering, principle component analysis (**PCA**), and **neural networks** with **backpropagation**. Each notebook demonstrates practical implementation and application of these algorithms on various datasets such as the **California Housing** Dataset, **MNIST** dataset, **Iris** dataset, **Auto-MPG** dataset, and the **UCI Adult Census Income** dataset. Additionally, it covers topics like **gradient descent optimization**, model evaluation metrics (e.g., **accuracy, precision, recall, f1 score**), **regularization** techniques (e.g., **Lasso**, **Ridge**), and **data visualization**.\n\n| Title                                                                                                                   | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n| ----------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [01 - Intro to Numpy](https://www.kaggle.com/code/sacrum/ml-labs-01-intro-to-numpy)                                     | The notebook demonstrates NumPy's efficiency for mathematical operations like array `reshaping`, `sigmoid`, `softmax`, `dot` and `outer products`, `L1 and L2 losses`, and matrix operations. It highlights NumPy's superiority over standard Python lists in speed and convenience for scientific computing and machine learning tasks.                                                                                                                                                                                              |\n| [02 - Linear Regression From Scratch](https://www.kaggle.com/code/sacrum/ml-labs-02-linear-regression-from-scratch)     | This notebook implements `linear regression` and `gradient descent` from scratch in Python using `NumPy`, focusing on predicting house prices with the `California Housing Dataset`. It defines functions for prediction, `MSE` calculation, and gradient computation. Batch gradient descent is used for optimization. The dataset is loaded, scaled, and split. `Batch, stochastic, and mini-batch gradient descents` are applied with varying hyperparameters. Finally, the MSEs of the predictions from each method are compared. |\n| [03 - Logistic Regression from Scratch](https://www.kaggle.com/code/sacrum/ml-labs-03-logistic-regression-from-scratch) | This notebook outlines the implementation of `logistic regression` from scratch in Python using `NumPy`, including functions for prediction, loss calculation, gradient computation, and batch `gradient descent` optimization, applied to the `MNIST` dataset for handwritten digit recognition and `Iris` data. And also inclues metrics like `accuracy`, `precision`, `recall`, `f1 score`                                                                                                                                         |\n| [04 - Auto-MPG Regression](https://www.kaggle.com/code/sacrum/ml-labs-04-auto-mpg-regression)                           | The notebook uses `pandas` for data manipulation, `seaborn` and `matplotlib` for visualization, and `sklearn` for `linear regression` and `regularization` techniques (`Lasso` and `Ridge`). It includes data loading, processing, visualization, model training, and evaluation on the `Auto-MPG dataset`.                                                                                                                                                                                                                           |\n| [05 - Desicion Trees from Scratch](https://www.kaggle.com/code/sacrum/ml-labs-05-desicion-trees-from-scratch)           | In this notebook, `DecisionTree` algorithm has been implmented from scratch and applied on dummy dataset                                                                                                                                                                                                                                                                                                                                                                                                                              |\n| [06 - KNN from Scratch](https://www.kaggle.com/code/sacrum/ml-labs-06-knn-from-scratch)                                 | In this notebook, `K-Nearest Neighbour` algorithm has been implemented from scratch and compared with KNN provided in scikit-learn package                                                                                                                                                                                                                                                                                                                                                                                            |\n| [07 - SVM](https://www.kaggle.com/code/sacrum/ml-labs-07-svm)                                                           | This notebook implements `SVM classifier` on `Iris Dataset`                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n| [08 - Naive Bayes](https://www.kaggle.com/code/sacrum/ml-labs-08-naive-bayes)                                           | This notebook trains `Naive Bayes` and compares it with other algorithms `Decision Trees`, `SVM` and `Logistic Regression`                                                                                                                                                                                                                                                                                                                                                                                                            |\n| [09 - K-means](https://www.kaggle.com/code/sacrum/ml-labs-09-k-means)                                                   | In this notebook `K-means` algorithm has been implemented using `scikit-learn` and different values of `k` are compared to understand the `elbow method` in `Calinski Harabasz Scores`                                                                                                                                                                                                                                                                                                                                                |\n| [10 - UCI Adult Census Income](https://www.kaggle.com/code/sacrum/ml-labs-10-uci-adult-census-income)                   | Here I have used the UCI Adult Income dataset and applied different machine learning algorithms to find the best model configuration for predicting salary from the given information                                                                                                                                                                                                                                                                                                                                                 |\n| [11 - PCA](https://www.kaggle.com/code/sacrum/ml-labs-11-pca)                                                           | `Principle Component Analysis` implemented from scratch                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| [12 - Neural Networks](https://www.kaggle.com/code/sacrum/ml-labs-12-neural-networks)                                   | This code implements neural networks with back propagation from scratch                                                                                                                                                                                                                                                                                                                                                                                                                                                               |","metadata":{}}]}